{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "import googleapiclient.discovery\n",
    "import datetime\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import methods #file containing functions to interact with Google calendar like add event, create event, etc\n",
    "import json\n",
    "import re\n",
    "\n",
    "google_calendar = methods.get_google_calendar_service() #get the google calendar service\n",
    "tasks = methods.get_task_service() #get the task service\n",
    "tasks_list = methods.extractTasks(tasks)\n",
    "calendar = methods.extractCalendar(google_calendar) #extract the calendar from the service\n",
    "now = methods.current_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-FDU1qeE4WbHsR6wjsn31T3BlbkFJdmIXHoDKzvAcuDyyQAE3\"\n",
    "client = OpenAI()\n",
    "client.api_key = \"sk-proj-FDU1qeE4WbHsR6wjsn31T3BlbkFJdmIXHoDKzvAcuDyyQAE3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_gpt(client, messages):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo-preview\",\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conversation():\n",
    "    filename = f\"activity_log.txt\"\n",
    "    try:\n",
    "        with open(filename, \"r\") as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        return \"\"\n",
    "    \n",
    "conversation = get_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = '''You are an expert in evaluating the responses of LLMs. Given the transcript of a conversation between a user and an LLM assistant, along with the system's prompts, evaluate and output the following metrics on a scale of 1-10:\n",
    "Accuracy: How accurate is the assistant's response in the context of the user's prompt? Does the assistant correctly understand the user's intent?\n",
    "Relevance: How relevant is the assistant response to the user's prompt?\n",
    "Completeness: Do the responses fully address all parts of the user's request?\n",
    "Coherence: How coherent is the assistant's response?\n",
    "Consistency: Are the responses are consistent over time and do not contradict previous responses, especially in ongoing dialogues?\n",
    "Engagement: How engaging and captivating are the responses? Can the assistant maintain the user's interest?\n",
    "Tonality: Are the LLM assistant's responses polite and friendly?\n",
    "Robustness: How well does the assistant handle edge cases or ambiguous inputs? The assistant should either respond appropriately or seek clarification without causing confusion or errors.\n",
    "Overall: Overall, how would you rate the assistant's performance?\n",
    "Take your time to read the entire transcript and think and evaluate the responses. Then output the scores. Do not provide feedback. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 8\n",
      "Relevance: 9\n",
      "Completeness: 8\n",
      "Coherence: 9\n",
      "Consistency: 9\n",
      "Engagement: 8\n",
      "Tonality: 9\n",
      "Robustness: 7\n",
      "Overall: 8\n"
     ]
    }
   ],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": conversation}, \n",
    "            {\"role\": \"user\", \"content\": instruction}]\n",
    "response = chat_gpt(client, messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_llm_output(output, model):\n",
    "    \"\"\" Parses LLM output and adds an iteration number. \"\"\"\n",
    "    # Initialize an empty dictionary to store the metrics\n",
    "    metrics = {'Model': model}\n",
    "\n",
    "    # Split the output into lines and iterate over them\n",
    "    for line in output.strip().split('\\n'):\n",
    "        key, value = line.split(':')\n",
    "        key = key.strip()\n",
    "        value = value.strip()\n",
    "        \n",
    "        # Handle non-numeric entries specially\n",
    "        if value == \"N/A\":\n",
    "            metrics[key] = None  # Use None to represent missing values in pandas\n",
    "        else:\n",
    "            metrics[key] = float(value)\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Relevance</th>\n",
       "      <th>Completeness</th>\n",
       "      <th>Coherence</th>\n",
       "      <th>Consistency</th>\n",
       "      <th>Engagement</th>\n",
       "      <th>Tonality</th>\n",
       "      <th>Robustness</th>\n",
       "      <th>Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT-3.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Model  Accuracy  Relevance  Completeness  Coherence  Consistency  \\\n",
       "0  GPT-3.5       8.0        9.0           8.0        9.0          9.0   \n",
       "\n",
       "   Engagement  Tonality  Robustness  Overall  \n",
       "0         8.0       9.0         7.0      8.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = parse_llm_output(response, \"GPT-3.5\")\n",
    "df = pd.DataFrame([metrics])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"model_comparison.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
